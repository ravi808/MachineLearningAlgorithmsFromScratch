{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kh8A-sd8uV-d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ac5uZ2sqaPAk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "class linear_regression:\n",
    "\n",
    "  def generator(self,n,m,sigma): #m rows and n+1 we\n",
    "      \n",
    "      x = np.random.rand(n, m+1) # INPUT data\n",
    "     \n",
    "      e=np.random.normal(0,sigma,(n,1))# gaussian distribution\n",
    "\n",
    "      x[:,0]=1\n",
    "     \n",
    "      beta=np.random.rand(m+1,1)\n",
    "      \n",
    "      y=np.matmul(x,beta)+e\n",
    "     \n",
    "      return x,y,beta\n",
    "\n",
    "  def cost(self,pred_y,y,n):\n",
    "      cost=(1/(2*n))*np.sum((pred_y-y)**2)\n",
    "      return cost\n",
    "\n",
    "  \n",
    "\n",
    "  def gradient(self,x,y,pred_y,n):\n",
    "      gradient=(-2/n)*np.matmul((y-pred_y).T,x)\n",
    "      return gradient\n",
    "\n",
    "\n",
    "  def regression(self,x,y,epochs,threshold):\n",
    "      learning_rate=0.001\n",
    "\n",
    "      n=x.shape[0]\n",
    "      m=x.shape[1]\n",
    "\n",
    "      weight=np.random.rand(m,1)*learning_rate\n",
    "\n",
    "      \n",
    "      pre_cost=float('inf') #initial cost\n",
    "\n",
    "      for i in range(epochs):\n",
    "\n",
    "          pred_y = np.matmul(x,weight)\n",
    "          cost=self.cost(pred_y,y,n)\n",
    "\n",
    "          if abs(pre_cost-cost).all()<=threshold:\n",
    "              break\n",
    "\n",
    "          pre_cost=cost  #updating the cost value\n",
    "\n",
    "          gd=self.gradient(x,y,pred_y,n)# finding gradient\n",
    "\n",
    "          weight=weight-learning_rate*gd # updating weights\n",
    "          \n",
    "      return pre_cost,weight\n",
    "\n",
    "obj1=linear_regression()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0jNz92CSy-29",
    "outputId": "0f73f6ac-2634-4f0b-9d6d-292a36c2da1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.466679153913006\n"
     ]
    }
   ],
   "source": [
    "# solution1\n",
    "X,y,beta=obj1.generator(2,4,10) # ex standard deviation 1 \n",
    "\n",
    "#solution2 \n",
    "finalcost,weight=obj1.regression(X,y,500,0.002) #500 iterations and 0.02 threshold\n",
    "\n",
    "print(finalcost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SkMwWNXyyE7G"
   },
   "source": [
    "# Helpful Links\n",
    "1. Random Vs Normal Distribution [here](https://www.geeksforgeeks.org/rand-vs-normal-numpy-random-python/#:~:text=numpy.-,random.,because%20of%20its%20characteristics%20shape.)\n",
    "\n",
    "\n",
    "2. Gradient Descent [here](https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Linear regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
